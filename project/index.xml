<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Projects | Xiaowei Zhang</title>
    <link>https://xiaoweiz.github.io/project/</link>
      <atom:link href="https://xiaoweiz.github.io/project/index.xml" rel="self" type="application/rss+xml" />
    <description>Projects</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><copyright>Â© 2021</copyright><lastBuildDate>Sat, 27 Feb 2021 10:13:49 +0800</lastBuildDate>
    <image>
      <url>https://xiaoweiz.github.io/media/icon_hua2ec155b4296a9c9791d015323e16eb5_11927_512x512_fill_lanczos_center_3.png</url>
      <title>Projects</title>
      <link>https://xiaoweiz.github.io/project/</link>
    </image>
    
    <item>
      <title>Causal Reinforcement Learning</title>
      <link>https://xiaoweiz.github.io/project/causal-reinforcement-learning/</link>
      <pubDate>Sat, 27 Feb 2021 10:13:49 +0800</pubDate>
      <guid>https://xiaoweiz.github.io/project/causal-reinforcement-learning/</guid>
      <description>&lt;p&gt;In the standard data analysis framework, data is first collected (once for all), and then data analysis is carried out. Moreover, the data-generating process is typically assumed to be exogenous. This approach is natural when the data analyst has no impact on how the data is generated. The advancement of digital technology, however, has facilitated firms to learn from data and make decisions at the same time. As these decisions generate new data, the data analyst&amp;mdash;a business manager or an algorithm&amp;mdash;also becomes the data generator.
This interaction generates a new type of bias&amp;mdash;&lt;em&gt;reinforcement bias&lt;/em&gt;&amp;mdash;that exacerbates the endogeneity problem in static data analysis. Causal inference techniques ought to be incorporated into reinforcement learning to address such issues.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Simulation Optimization</title>
      <link>https://xiaoweiz.github.io/project/simulation-optimization/</link>
      <pubDate>Mon, 03 Jun 2019 21:26:25 +0800</pubDate>
      <guid>https://xiaoweiz.github.io/project/simulation-optimization/</guid>
      <description>&lt;p&gt;Modern stochastic systems often have a sophisticated structure. The system performance is generally not an analytical function of the decision variables, but a complex surface that can only be evaluated at discrete locations via noisy samples, usually from a simulation model. Since the sampling process is often expensive, we would like to identify the optimal decision with minimal samples. Simulation optimization is essentially a trade-off between &lt;em&gt;exploitation&lt;/em&gt;, which tends to sample more  at &amp;lsquo;&amp;lsquo;promising&amp;rsquo;&amp;rsquo; areas, and &lt;em&gt;exploration&lt;/em&gt;, which tends to sample more at &amp;lsquo;&amp;lsquo;uncharted&amp;rsquo;&amp;rsquo; areas.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Simulation Metamodeling</title>
      <link>https://xiaoweiz.github.io/project/simulation-metamodeling/</link>
      <pubDate>Mon, 03 Jun 2019 20:22:36 +0800</pubDate>
      <guid>https://xiaoweiz.github.io/project/simulation-metamodeling/</guid>
      <description>&lt;p&gt;Simulation models are extensively used in a great variety of areas including health care, finance, manufacturing, logistics, supply chain management, telecommunication, etc. to facilitate related decision making processes. However, simulation models are usually computationally expensive to execute, which severely restricts the usefulness of simulation in settings such as real-time decision making and system optimization. Metamodeling is technique that has been actively developed in the simulation community, in order to alleviate the  inefficiency issue. The basic idea is that the user executes the simulation model only at a small number of carefully selected &amp;lsquo;&amp;lsquo;design points&amp;rsquo;&amp;rsquo;. A metamodel&amp;mdash;a statistical model of the simulation model&amp;mdash;can be built to approximate the true response surface by properly interpolating the simulation outputs. The response at other design points is then predicted by the metamodel without running simulation at all, thereby substantially reducing the computational costs.  The metamodel can be used to efficiently search for the optimal values of the design variables, even in real time.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Arrival Data Modeling</title>
      <link>https://xiaoweiz.github.io/project/arrival-data-modeling/</link>
      <pubDate>Mon, 03 Jun 2019 19:25:36 +0800</pubDate>
      <guid>https://xiaoweiz.github.io/project/arrival-data-modeling/</guid>
      <description>&lt;p&gt;Operational decision making in service systems often depends largely on the characterization of the random fluctuations involved. Exogenous arrivals represent a primary source of uncertainty and their stochastic behavior needs to be modeled carefully. Recent empirical studies demonstrate that the arrival data usually exhibits a dynamics that is substantially more sophisticated than the Poisson process model (and even the renewal process model), both of which are the underlying assumption of a wide variety of results in the queueing theory.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Affine Models in Finance</title>
      <link>https://xiaoweiz.github.io/project/affine-models-in-finance/</link>
      <pubDate>Mon, 03 Jun 2019 16:24:55 +0800</pubDate>
      <guid>https://xiaoweiz.github.io/project/affine-models-in-finance/</guid>
      <description>&lt;p&gt;Affine processes constitute an important class of continuous time stochastic models that are widely used in finance and econometrics due to their modeling flexibility and computational/analytical tractability. This class of models include affine jump-diffusions (AJDs) and affine point processes (APPs). Examples of AJDs include the Ornstein-Uhlenbeck (OU) process (i.e. the Vasicek model), the square-root diffusion process, (i.e. the Cox-Ingersoll-Ross model), and the Heston stochastic volatility model, all of which are classical models in dynamic asset pricing. On the other hand, examples of APPs include the Hawkes process and its multidimensional extensions, which was applied in credit risk modeling and has lately found important applications in high-frequency trading.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Rare Event Analysis</title>
      <link>https://xiaoweiz.github.io/project/rare-event-analysis/</link>
      <pubDate>Sun, 03 Jun 2018 12:01:07 +0800</pubDate>
      <guid>https://xiaoweiz.github.io/project/rare-event-analysis/</guid>
      <description>&lt;p&gt;Rare events refer to those that occur with small probabilities but would have substantial impact when they do. Examples include meltdown of a communication network and credit default of a large financial institution. A typical approach to rare event analysis is the large deviations (LD) technique, which characterizes the rare event probability up to the correct logarithmic order of magnitude. To obtain a more accurate estimate, one often resorts to Monte Carlo (MC) simulation. But using the plain vanilla MC to estimate small probabilities requires an enormous number of samples due to the fact that the variance of the MC estimator is orders of magnitude larger than its mean. The LD technique can facilitate to develop importance sampling schemes that reduce the variance of the MC estimator dramatically.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
